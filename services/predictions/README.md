# Predictions

The Predictions service is what handles making project -> contributor and person -> skill links.

It is a Flask-based API that takes in raw data from the Backend service to form predictions using machine learning and heuristic models.

The Predictions service can look rather simple from the surface since it just looks like "take data in, process it, run through a model, return results", but there are a number of nuances to each of these steps.

# Tech Stack

- Language: Python
- API Framework: Flask
- Machine Learning Model Framework: Scikit-Learn

# Code Structure

```
├── Dockerfile				# Docker configuration
├── gunicorn.conf.py		# Gunicorn (web server) configuration
├── Makefile				# Makefile with commands for Kubails
├── Pipfile					# Python dependencies
├── Pipfile.lock			# Lock file for Python dependencies
├── setup.cfg				# Configuration for the various dev-dependencies (linter, etc)
│
└── src/					# Source files
    ├── config.py			# Service configuration; where environment variables get pulled in
    ├── controllers/		# Flask controller files; see the below section __Controllers Setup__ for how these work
    ├── data_models/        # Regular old data models represented using classes (i.e. not ML models)
    ├── main.py             # Entrypoint to the API
    ├── middleware/         # Other functions that can be used as middleware
    ├── predictors/         # Classes that wrap the models to perform predictions
    ├── test_dummy.py       # Dummy test class so that the tests don't fail in the CI pipeline when there are no tests (note: there are no tests)
    ├── trained_models/     # Storage spot for putting trained and serialized Scikit-Learn (or other) models
    ├── training/           # Scripts for training the Scikit-Learn (or other) models
    └── utils/              # Other utilities for the API
```

## Controllers Setup

The Flask controllers are setup using a custom `NestableBlueprint`.

This custom class allows regular Flask blueprints to be nested inside one another in a folder hierarchy, so that each folder is independent from other parent or sibling folders; that is, it enables modularity.

You can take any controller folder and move it to any other folder (and register the controllers in the new folder), and it should just work.

This also gives us clean, autogenerated routes. The default controller hierarchy is `/api/v1/`, which corresponds to the folder hierarchy in the `controllers` folder.
Anything under the `v1` folder would then be registered under `/api/v1/`.

# Interactions with Other Services

The following explains how the Predictions service interacts with the other Skillhub services.

## Backend

The Predictions service gets called by the Backend to perform predictions for contributors and skills (i.e. whether or not a person is a 'contributor' for a project, and whether or not a person has a skill).

The Predictions service then returns the predicted results and the Backend persists them.

## Frontend

No interactions.

## Scraper

No interactions.

# Secrets

The following are the secrets stored in `service-secrets.zip` (or `.env.encrypted`). These are exposed to the service as environment variables.

- SKILLHUB_API_KEY: An arbitrary string used as an API key for the Backend service. Must match the Backend service's `PREDICTIONS_API_KEY`.

# Running the Predictions Service

The `predictions` service handles taking in raw Jira data, processing it, and making various types of predictions using machine learning models.

Contributor predictions are made by looking at all of the issues for a given Jira project, and seeing if the ratio of various features (e.g. # of assigned tickets, # of comments made, etc) is high enough to roughly indicate that the user was a contributor to the project.

Obviously, the 'roughly' part comes into play since it is a machine learning model that is making the call.

Running the predictions service is just like any other service; it'll come up as part of `make start`.

## Predictions Service Authentication

In order to make any calls to the predictions service, you'll need the API key. When running locally, the value of the API key can be in `services/predictions/src/config.py`, under `SKILLHUB_API_KEY`.

To use the API key with curl, for example, you need to specify it as a header, like so:

```
curl -H "x-api-key: ${SKILLHUB_API_KEY}"
```

If you want to call the predictions service once it has been deployed to a branch environment or to production, please contact Devin to get access to the prod API key.

# Making a Contributor Prediction

In order to make a contributor prediction by hand, you first need to get a hold of some raw Jira issue data.

If you want to grab it manually from CDH Studio accessible Jira instance, you can do something like this:

```
curl -u "${YOUR JIRA USERNAME}:${YOUR JIRA PASSWORD}" https://jira.ised-isde.canada.ca/rest/api/2/search?jql=project=${A JIRA PROJECT KEY}&maxResults=1000&expand=changelog > data.json
```

where you subsitute in the correct information for your Jira username/password, as well as a Jira project key.

This'll only get you the first 1000 issues for the project, but that should be enough for just quick testing purposes.

Once you have your Jira data in a JSON file, you can then call the predictions service like so:

```
curl -X POST -H "x-api-key: ${SKILLHUB_API_KEY}" --data @data.json localhost:5002/api/v1/contributors/predict
```

# Model Training

If the models for the predictions ever need to be retrained, the following will show you how to do it.

## Contributors Model

Training the contributors model happens in two steps: picking/generating a dataset, and then training the model on that data set.

### Picking/Generating a Dataset

To find out which datasets already exist, look in the `services/predictions/src/training/training_data/` folder. If you want to use an existing dataset, make note of the 'hash' -- the part of the CSV file name before `--training-data.csv`.

If you instead want to generate a new dataset from the CDH Studio accessible [Jira instance](https://jira.ised-isde.canada.ca), you can run generation process with the following `make` command:

```
make generate-contributors-model-training-data JIRA_AUTH_TOKEN="YOUR_JIRA_USERNAME:YOUR_JIRA_PASSWORD"
```

This will add another CSV file to the `training_data` folder. Again, make note of its hash.

**NOTE 1:** One of the already existing datasets is the `our_jira_and_ccdev--training-data.csv` file. This dataset is special because it includes, as the name implies, all of the data from our [Jira instance](https://jira.ised-isde.canada.ca), as well as the data from the `CCDEV` project from the on-network [Jira instance](http://jira.ic.gc.ca).

This is the dataset that was used to train the current contributors model.

**NOTE 2:**: Since running `make generate-contributors-model-training-data` runs `scraper` and `predictions` service scripts on your host machine (_not_ in a Docker container), you'll need to make sure to have appropriate installations of Node/npm and Python 3/pipenv to install the packages and run the scripts.

### Training the Model

Once you have your dataset hash from the last section, training the model is just another `make` command:

```
make train-contributors-model DATASET_HASH="YOUR_DATASET_HASH"
```

On a 4 core i5-6200U laptop, this took about 4.5 minutes to train. It runs the grid search in parallel, so more cores = faster training.

Once it's done, it'll output the trained model to `services/predictions/src/trained_models/`.

To make use of the trained model in the `predictions` service, you'll need to manually update the `CONTRIBUTORS_MODEL` variable in the `services/predictions/src/config.py` file to use the new file name.
